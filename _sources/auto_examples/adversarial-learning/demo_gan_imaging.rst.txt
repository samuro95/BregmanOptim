
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/adversarial-learning/demo_gan_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_adversarial-learning_demo_gan_imaging.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_adversarial-learning_demo_gan_imaging.py:


Imaging inverse problems with adversarial networks
==================================================

This example shows you how to train various networks using adversarial
training for deblurring problems. We demonstrate running training and
inference using a conditional GAN (i.e. DeblurGAN), CSGM, AmbientGAN and
UAIR implemented in the library, and how to simply train
your own GAN by using :meth:`deepinv.training.AdversarialTrainer`. These
examples can also be easily extended to train more complicated GANs such
as CycleGAN.

This example is based on the following papers:

-  Kupyn et al., `DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks <https://openaccess.thecvf.com/content_cvpr_2018/papers/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.pdf>`_
-  Bora et al., `Compressed Sensing using Generative
   Models <https://arxiv.org/abs/1703.03208>`_ (CSGM)
-  Bora et al., `AmbientGAN: Generative models from lossy
   measurements <https://openreview.net/forum?id=Hy7fDog0b>`_
-  Pajot et al., `Unsupervised Adversarial Image
   Reconstruction <https://openreview.net/forum?id=BJg4Z3RqF7>`_

Adversarial networks are characterised by the addition of an adversarial
loss :math:`\mathcal{L}_\text{adv}` to the standard reconstruction loss:

.. math:: \mathcal{L}_\text{adv}(x,\hat x;D)=\mathbb{E}_{x\sim p_x}\left[q(D(x))\right]+\mathbb{E}_{\hat x\sim p_{\hat x}}\left[q(1-D(\hat x))\right]

where :math:`D(\cdot)` is the discriminator model, :math:`x` is the
reference image, :math:`\hat x` is the estimated reconstruction,
:math:`q(\cdot)` is a quality function (e.g :math:`q(x)=x` for WGAN).
Training alternates between generator :math:`G` and discriminator
:math:`D` in a minimax game. When there are no ground truths (i.e.
unsupervised), this may be defined on the measurements :math:`y`
instead.

.. GENERATED FROM PYTHON SOURCE LINES 37-50

.. code-block:: Python


    import deepinv as dinv
    from deepinv.loss import adversarial
    from deepinv.physics.generator import MotionBlurGenerator
    import torch
    from torch.utils.data import DataLoader, random_split
    from torchvision.datasets import ImageFolder
    from torchvision.transforms import Compose, ToTensor, CenterCrop, Resize
    from torchvision.datasets.utils import download_and_extract_archive

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 51-58

Generate dataset
~~~~~~~~~~~~~~~~
In this example we use the Urban100 dataset resized to 128x128. We apply random
motion blur physics using
:meth:`deepinv.physics.generator.MotionBlurGenerator`, and save the data
using :meth:`deepinv.datasets.generate_dataset`.


.. GENERATED FROM PYTHON SOURCE LINES 58-89

.. code-block:: Python


    physics = dinv.physics.Blur(padding="circular", device=device)
    blur_generator = MotionBlurGenerator((11, 11))

    dataset = dinv.datasets.Urban100HR(
        root="Urban100",
        download=True,
        transform=Compose([ToTensor(), Resize(256), CenterCrop(128)]),
    )

    train_dataset, test_dataset = random_split(dataset, (0.8, 0.2))

    # Generate data pairs x,y offline using a physics generator
    dataset_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        physics_generator=blur_generator,
        device=device,
        save_dir="Urban100",
        batch_size=1,
    )

    train_dataloader = DataLoader(
        dinv.datasets.HDF5Dataset(dataset_path, train=True), shuffle=True
    )
    test_dataloader = DataLoader(
        dinv.datasets.HDF5Dataset(dataset_path, train=False), shuffle=False
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/135388067 [00:00<?, ?it/s]      9%|▉         | 11.3M/129M [00:00<00:01, 119MB/s]     19%|█▉        | 24.2M/129M [00:00<00:00, 128MB/s]     29%|██▉       | 37.5M/129M [00:00<00:00, 133MB/s]     40%|████      | 51.8M/129M [00:00<00:00, 139MB/s]     52%|█████▏    | 67.3M/129M [00:00<00:00, 148MB/s]     63%|██████▎   | 81.4M/129M [00:00<00:00, 142MB/s]     74%|███████▎  | 95.1M/129M [00:00<00:00, 140MB/s]     84%|████████▍ | 108M/129M [00:00<00:00, 141MB/s]      95%|█████████▍| 123M/129M [00:00<00:00, 142MB/s]    100%|██████████| 129M/129M [00:00<00:00, 140MB/s]
    Extracting:   0%|          | 0/101 [00:00<?, ?it/s]    Extracting:  16%|█▌        | 16/101 [00:00<00:00, 148.66it/s]    Extracting:  33%|███▎      | 33/101 [00:00<00:00, 157.66it/s]    Extracting:  52%|█████▏    | 53/101 [00:00<00:00, 173.47it/s]    Extracting:  70%|███████   | 71/101 [00:00<00:00, 161.37it/s]    Extracting:  87%|████████▋ | 88/101 [00:00<00:00, 159.30it/s]    Extracting: 100%|██████████| 101/101 [00:00<00:00, 159.02it/s]
    Dataset has been successfully downloaded.
    Dataset has been saved in Urban100




.. GENERATED FROM PYTHON SOURCE LINES 90-100

Define models
~~~~~~~~~~~~~

We first define reconstruction network (i.e conditional generator) and
discriminator network to use for adversarial training. For demonstration
we use a simple U-Net as the reconstruction network and the
discriminator from `PatchGAN <https://arxiv.org/abs/1611.07004>`_, but
these can be replaced with any architecture e.g transformers, unrolled
etc. Further discriminator models are in :ref:`adversarial models <adversarial-networks>`.


.. GENERATED FROM PYTHON SOURCE LINES 100-127

.. code-block:: Python



    def get_models(model=None, D=None, lr_g=1e-4, lr_d=1e-4, device=device):
        if model is None:
            model = dinv.models.UNet(
                in_channels=3,
                out_channels=3,
                scales=2,
                circular_padding=True,
                batch_norm=False,
            ).to(device)

        if D is None:
            D = dinv.models.PatchGANDiscriminator(n_layers=2, batch_norm=False).to(device)

        optimizer = dinv.training.adversarial.AdversarialOptimizer(
            torch.optim.Adam(model.parameters(), lr=lr_g, weight_decay=1e-8),
            torch.optim.Adam(D.parameters(), lr=lr_d, weight_decay=1e-8),
        )
        scheduler = dinv.training.adversarial.AdversarialScheduler(
            torch.optim.lr_scheduler.StepLR(optimizer.G, step_size=5, gamma=0.9),
            torch.optim.lr_scheduler.StepLR(optimizer.D, step_size=5, gamma=0.9),
        )

        return model, D, optimizer, scheduler









.. GENERATED FROM PYTHON SOURCE LINES 128-148

Conditional GAN training
~~~~~~~~~~~~~~~~~~~~~~~~

Conditional GANs (Kupyn et al., `DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks
<https://openaccess.thecvf.com/content_cvpr_2018/papers/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.pdf>`_)
are a type of GAN where the generator is conditioned on a label or input. In the context of imaging,
this can be used to generate images from a given measurement. In this example, we use a simple U-Net as the generator
and a PatchGAN discriminator. The forward pass of the generator is given by:

**Conditional GAN** forward pass:

.. math:: \hat x = G(y)

**Conditional GAN** loss:

.. math:: \mathcal{L}=\mathcal{L}_\text{sup}(\hat x, x)+\mathcal{L}_\text{adv}(\hat x, x;D)

where :math:`\mathcal{L}_\text{sup}` is a supervised loss such as
pixel-wise MSE or VGG Perceptual Loss.


.. GENERATED FROM PYTHON SOURCE LINES 148-152

.. code-block:: Python


    G, D, optimizer, scheduler = get_models()









.. GENERATED FROM PYTHON SOURCE LINES 153-157

We next define pixel-wise and adversarial losses as defined above. We use the
MSE for the supervised pixel-wise metric for simplicity but this can be
easily replaced with a perceptual loss if desired.


.. GENERATED FROM PYTHON SOURCE LINES 157-165

.. code-block:: Python


    loss_g = [
        dinv.loss.SupLoss(metric=torch.nn.MSELoss()),
        adversarial.SupAdversarialGeneratorLoss(device=device),
    ]
    loss_d = adversarial.SupAdversarialDiscriminatorLoss(device=device)









.. GENERATED FROM PYTHON SOURCE LINES 166-172

We are now ready to train the networks using :meth:`deepinv.training.AdversarialTrainer`.
We load the pretrained models that were trained in the exact same way after 50 epochs,
and fine-tune the model for 1 epoch for a quick demo.
You can find the pretrained models on HuggingFace https://huggingface.co/deepinv/adversarial-demo.
To train from scratch, simply comment out the model loading code and increase the number of epochs.


.. GENERATED FROM PYTHON SOURCE LINES 172-201

.. code-block:: Python


    ckpt = torch.hub.load_state_dict_from_url(
        dinv.models.utils.get_weights_url("adversarial-demo", "deblurgan_model.pth"),
        map_location=lambda s, _: s,
    )

    G.load_state_dict(ckpt["state_dict"])
    D.load_state_dict(ckpt["state_dict_D"])
    optimizer.load_state_dict(ckpt["optimizer"])

    trainer = dinv.training.AdversarialTrainer(
        model=G,
        D=D,
        physics=physics,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=1,
        losses=loss_g,
        losses_d=loss_d,
        optimizer=optimizer,
        scheduler=scheduler,
        verbose=True,
        show_progress_bar=False,
        save_path=None,
        device=device,
    )

    G = trainer.train()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/adversarial-demo/resolve/main/deblurgan_model.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/deblurgan_model.pth
      0%|          | 0.00/12.7M [00:00<?, ?B/s]      9%|▉         | 1.12M/12.7M [00:00<00:01, 10.6MB/s]     18%|█▊        | 2.25M/12.7M [00:00<00:00, 11.1MB/s]     27%|██▋       | 3.38M/12.7M [00:00<00:00, 10.3MB/s]     34%|███▍      | 4.38M/12.7M [00:00<00:00, 10.3MB/s]     42%|████▏     | 5.38M/12.7M [00:00<00:00, 10.3MB/s]     50%|█████     | 6.38M/12.7M [00:00<00:00, 10.3MB/s]     58%|█████▊    | 7.38M/12.7M [00:00<00:00, 10.4MB/s]     66%|██████▌   | 8.38M/12.7M [00:00<00:00, 10.3MB/s]     74%|███████▍  | 9.38M/12.7M [00:00<00:00, 10.4MB/s]     82%|████████▏ | 10.4M/12.7M [00:01<00:00, 10.4MB/s]     90%|████████▉ | 11.4M/12.7M [00:01<00:00, 10.4MB/s]     97%|█████████▋| 12.4M/12.7M [00:01<00:00, 10.4MB/s]    100%|██████████| 12.7M/12.7M [00:01<00:00, 10.5MB/s]
    The model has 444867 trainable parameters
    Train epoch 0: SupLoss=0.004, SupAdversarialGeneratorLoss=0.003, TotalLoss=0.006, PSNR=25.826
    Eval epoch 0: PSNR=25.339




.. GENERATED FROM PYTHON SOURCE LINES 202-204

Test the trained model and plot the results. We compare to the pseudo-inverse as a baseline.


.. GENERATED FROM PYTHON SOURCE LINES 204-209

.. code-block:: Python


    trainer.plot_images = True
    trainer.test(test_dataloader)





.. image-sg:: /auto_examples/adversarial-learning/images/sphx_glr_demo_gan_imaging_001.png
   :alt: Ground truth, Measurement, No learning, Reconstruction
   :srcset: /auto_examples/adversarial-learning/images/sphx_glr_demo_gan_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Eval epoch 0: PSNR=25.339, PSNR no learning=22.129
    Test results:
    PSNR no learning: 22.129 +- 2.703
    PSNR: 25.339 +- 3.741

    {'PSNR no learning': np.float64(22.128802490234374), 'PSNR no learning_std': np.float64(2.703303237720839), 'PSNR': np.float64(25.33935546875), 'PSNR_std': np.float64(3.7408121787192714)}



.. GENERATED FROM PYTHON SOURCE LINES 210-228

UAIR training
~~~~~~~~~~~~~

Unsupervised Adversarial Image Reconstruction (UAIR) (Pajot et al.,
`Unsupervised Adversarial Image Reconstruction <https://openreview.net/forum?id=BJg4Z3RqF7>`_)
is a method for solving inverse problems using generative models. In this
example, we use a simple U-Net as the generator and discriminator, and
train using the adversarial loss. The forward pass of the generator is defined as:

**UAIR** forward pass:

.. math:: \hat x = G(y),

**UAIR** loss:

.. math:: \mathcal{L}=\mathcal{L}_\text{adv}(\hat y, y;D)+\lVert \forw{\inverse{\hat y}}- \hat y\rVert^2_2,\quad\hat y=\forw{\hat x}.

We next load the models and construct losses as defined above.

.. GENERATED FROM PYTHON SOURCE LINES 228-237

.. code-block:: Python


    G, D, optimizer, scheduler = get_models(
        lr_g=1e-4, lr_d=4e-4
    )  # learning rates from original paper

    loss_g = adversarial.UAIRGeneratorLoss(device=device)
    loss_d = adversarial.UnsupAdversarialDiscriminatorLoss(device=device)









.. GENERATED FROM PYTHON SOURCE LINES 238-242

We are now ready to train the networks using :meth:`deepinv.training.AdversarialTrainer`.
Like above, we load a pretrained model trained in the exact same way for 50 epochs,
and fine-tune here for a quick demo with 1 epoch.


.. GENERATED FROM PYTHON SOURCE LINES 242-270

.. code-block:: Python


    ckpt = torch.hub.load_state_dict_from_url(
        dinv.models.utils.get_weights_url("adversarial-demo", "uair_model.pth"),
        map_location=lambda s, _: s,
    )

    G.load_state_dict(ckpt["state_dict"])
    D.load_state_dict(ckpt["state_dict_D"])
    optimizer.load_state_dict(ckpt["optimizer"])

    trainer = dinv.training.AdversarialTrainer(
        model=G,
        D=D,
        physics=physics,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=1,
        losses=loss_g,
        losses_d=loss_d,
        optimizer=optimizer,
        scheduler=scheduler,
        verbose=True,
        show_progress_bar=False,
        save_path=None,
        device=device,
    )
    G = trainer.train()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/adversarial-demo/resolve/main/uair_model.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/uair_model.pth
      0%|          | 0.00/12.7M [00:00<?, ?B/s]      2%|▏         | 256k/12.7M [00:00<00:08, 1.62MB/s]      4%|▍         | 512k/12.7M [00:00<00:07, 1.68MB/s]      6%|▌         | 768k/12.7M [00:00<00:07, 1.76MB/s]      8%|▊         | 1.00M/12.7M [00:00<00:06, 1.81MB/s]     10%|▉         | 1.25M/12.7M [00:00<00:06, 1.92MB/s]     12%|█▏        | 1.50M/12.7M [00:00<00:05, 1.96MB/s]     15%|█▍        | 1.88M/12.7M [00:00<00:05, 2.20MB/s]     18%|█▊        | 2.25M/12.7M [00:01<00:04, 2.50MB/s]     21%|██        | 2.62M/12.7M [00:01<00:03, 2.77MB/s]     24%|██▎       | 3.00M/12.7M [00:01<00:03, 2.80MB/s]     27%|██▋       | 3.38M/12.7M [00:01<00:03, 2.84MB/s]     30%|██▉       | 3.75M/12.7M [00:01<00:03, 2.86MB/s]     32%|███▏      | 4.12M/12.7M [00:01<00:03, 2.90MB/s]     35%|███▌      | 4.50M/12.7M [00:01<00:02, 3.02MB/s]     38%|███▊      | 4.88M/12.7M [00:02<00:02, 2.90MB/s]     41%|████▏     | 5.25M/12.7M [00:02<00:02, 2.93MB/s]     44%|████▍     | 5.62M/12.7M [00:02<00:02, 2.91MB/s]     47%|████▋     | 6.00M/12.7M [00:02<00:02, 2.85MB/s]     50%|█████     | 6.38M/12.7M [00:02<00:02, 2.73MB/s]     53%|█████▎    | 6.75M/12.7M [00:02<00:02, 2.51MB/s]     56%|█████▌    | 7.12M/12.7M [00:02<00:02, 2.54MB/s]     59%|█████▉    | 7.50M/12.7M [00:03<00:02, 2.59MB/s]     62%|██████▏   | 7.88M/12.7M [00:03<00:01, 2.68MB/s]     65%|██████▍   | 8.25M/12.7M [00:03<00:01, 2.73MB/s]     68%|██████▊   | 8.62M/12.7M [00:03<00:01, 2.80MB/s]     71%|███████   | 9.00M/12.7M [00:03<00:01, 2.82MB/s]     74%|███████▍  | 9.38M/12.7M [00:03<00:01, 2.79MB/s]     77%|███████▋  | 9.75M/12.7M [00:03<00:01, 2.85MB/s]     80%|███████▉  | 10.1M/12.7M [00:04<00:00, 2.77MB/s]     83%|████████▎ | 10.5M/12.7M [00:04<00:00, 2.71MB/s]     86%|████████▌ | 10.9M/12.7M [00:04<00:00, 2.58MB/s]     89%|████████▊ | 11.2M/12.7M [00:04<00:00, 2.57MB/s]     91%|█████████▏| 11.6M/12.7M [00:04<00:00, 2.56MB/s]     94%|█████████▍| 12.0M/12.7M [00:04<00:00, 2.53MB/s]     96%|█████████▋| 12.2M/12.7M [00:04<00:00, 2.53MB/s]     98%|█████████▊| 12.5M/12.7M [00:05<00:00, 2.47MB/s]    100%|██████████| 12.7M/12.7M [00:05<00:00, 2.59MB/s]
    The model has 444867 trainable parameters
    Train epoch 0: TotalLoss=0.143, PSNR=24.828
    Eval epoch 0: PSNR=24.388




.. GENERATED FROM PYTHON SOURCE LINES 271-273

Test the trained model and plot the results:


.. GENERATED FROM PYTHON SOURCE LINES 273-278

.. code-block:: Python


    trainer.plot_images = True
    trainer.test(test_dataloader)





.. image-sg:: /auto_examples/adversarial-learning/images/sphx_glr_demo_gan_imaging_002.png
   :alt: Ground truth, Measurement, No learning, Reconstruction
   :srcset: /auto_examples/adversarial-learning/images/sphx_glr_demo_gan_imaging_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Eval epoch 0: PSNR=24.388, PSNR no learning=22.129
    Test results:
    PSNR no learning: 22.129 +- 2.703
    PSNR: 24.388 +- 3.427

    {'PSNR no learning': np.float64(22.128802490234374), 'PSNR no learning_std': np.float64(2.703303237720839), 'PSNR': np.float64(24.388037109375), 'PSNR_std': np.float64(3.4268960671244293)}



.. GENERATED FROM PYTHON SOURCE LINES 279-310

CSGM / AmbientGAN training
~~~~~~~~~~~~~~~~~~~~~~~~~~

Compressed Sensing using Generative Models (CSGM) and AmbientGAN are two methods for solving inverse problems
using generative models. CSGM (Bora et al., `Compressed Sensing using Generative Models
<https://arxiv.org/abs/1703.03208>`_) uses a generative model to solve the inverse problem by optimising the latent
space of the generator. AmbientGAN (Bora et al., `AmbientGAN: Generative models from lossy measurements
<https://openreview.net/forum?id=Hy7fDog0b>`_) uses a generative model to solve the inverse problem by optimising the
measurements themselves. Both methods are trained using an adversarial loss; the main difference is that CSGM requires
a ground truth dataset (supervised loss), while AmbientGAN does not (unsupervised loss).

In this example, we use a DCGAN as the
generator and discriminator, and train using the adversarial loss. The forward pass of the generator is given by:

**CSGM** forward pass at train time:

.. math:: \hat x = \inverse{z},\quad z\sim \mathcal{N}(\mathbf{0},\mathbf{I}_k)

**CSGM**/**AmbientGAN** forward pass at eval time:

.. math:: \hat x = \inverse{\hat z}\quad\text{s.t.}\quad\hat z=\operatorname*{argmin}_z \lVert \forw{\inverse{z}}-y\rVert _2^2

**CSGM** loss:

.. math:: \mathcal{L}=\mathcal{L}_\text{adv}(\hat x, x;D)

**AmbientGAN** loss (where :math:`\forw{\cdot}` is the physics):

.. math:: \mathcal{L}=\mathcal{L}_\text{adv}(\forw{\hat x}, y;D)

We next load the models and construct losses as defined above.

.. GENERATED FROM PYTHON SOURCE LINES 310-328

.. code-block:: Python


    G = dinv.models.CSGMGenerator(
        dinv.models.DCGANGenerator(output_size=128, nz=100, ngf=32), inf_tol=1e-2
    ).to(device)
    D = dinv.models.DCGANDiscriminator(ndf=32).to(device)
    _, _, optimizer, scheduler = get_models(
        model=G, D=D, lr_g=2e-4, lr_d=2e-4
    )  # learning rates from original paper

    # For AmbientGAN:
    loss_g = adversarial.UnsupAdversarialGeneratorLoss(device=device)
    loss_d = adversarial.UnsupAdversarialDiscriminatorLoss(device=device)

    # For CSGM:
    loss_g = adversarial.SupAdversarialGeneratorLoss(device=device)
    loss_d = adversarial.SupAdversarialDiscriminatorLoss(device=device)









.. GENERATED FROM PYTHON SOURCE LINES 329-336

As before, we can now train our models. Since inference is very
slow for CSGM/AmbientGAN as it requires an optimisation, we only do one
evaluation at the end. Note the train PSNR is meaningless as this
generative model is trained on random latents.
Like above, we load a pretrained model trained in the exact same way for 50 epochs,
and fine-tune here for a quick demo with 1 epoch.


.. GENERATED FROM PYTHON SOURCE LINES 336-364

.. code-block:: Python


    ckpt = torch.hub.load_state_dict_from_url(
        dinv.models.utils.get_weights_url("adversarial-demo", "csgm_model.pth"),
        map_location=lambda s, _: s,
    )

    G.load_state_dict(ckpt["state_dict"])
    D.load_state_dict(ckpt["state_dict_D"])
    optimizer.load_state_dict(ckpt["optimizer"])

    trainer = dinv.training.AdversarialTrainer(
        model=G,
        D=D,
        physics=physics,
        train_dataloader=train_dataloader,
        epochs=1,
        losses=loss_g,
        losses_d=loss_d,
        optimizer=optimizer,
        scheduler=scheduler,
        verbose=True,
        show_progress_bar=False,
        save_path=None,
        device=device,
    )
    G = trainer.train()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/adversarial-demo/resolve/main/csgm_model.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/csgm_model.pth
      0%|          | 0.00/49.3M [00:00<?, ?B/s]      1%|▏         | 640k/49.3M [00:00<00:09, 5.32MB/s]      3%|▎         | 1.25M/49.3M [00:00<00:09, 5.20MB/s]      4%|▎         | 1.75M/49.3M [00:00<00:09, 5.04MB/s]      5%|▍         | 2.25M/49.3M [00:00<00:10, 4.79MB/s]      6%|▌         | 2.75M/49.3M [00:00<00:10, 4.68MB/s]      7%|▋         | 3.25M/49.3M [00:00<00:10, 4.73MB/s]      8%|▊         | 3.75M/49.3M [00:00<00:10, 4.74MB/s]      9%|▊         | 4.25M/49.3M [00:00<00:10, 4.53MB/s]     10%|▉         | 4.75M/49.3M [00:01<00:10, 4.60MB/s]     11%|█         | 5.25M/49.3M [00:01<00:10, 4.39MB/s]     12%|█▏        | 5.75M/49.3M [00:01<00:10, 4.37MB/s]     13%|█▎        | 6.25M/49.3M [00:01<00:10, 4.27MB/s]     14%|█▎        | 6.75M/49.3M [00:01<00:10, 4.31MB/s]     15%|█▍        | 7.25M/49.3M [00:01<00:10, 4.38MB/s]     16%|█▌        | 7.75M/49.3M [00:01<00:10, 4.31MB/s]     17%|█▋        | 8.25M/49.3M [00:01<00:10, 4.29MB/s]     18%|█▊        | 8.75M/49.3M [00:02<00:09, 4.32MB/s]     19%|█▉        | 9.25M/49.3M [00:02<00:10, 4.10MB/s]     20%|█▉        | 9.75M/49.3M [00:02<00:10, 4.12MB/s]     21%|██        | 10.2M/49.3M [00:02<00:09, 4.37MB/s]     22%|██▏       | 10.8M/49.3M [00:02<00:08, 4.54MB/s]     23%|██▎       | 11.2M/49.3M [00:02<00:08, 4.67MB/s]     24%|██▍       | 11.8M/49.3M [00:02<00:08, 4.64MB/s]     25%|██▍       | 12.2M/49.3M [00:02<00:08, 4.77MB/s]     26%|██▌       | 12.8M/49.3M [00:02<00:08, 4.73MB/s]     27%|██▋       | 13.2M/49.3M [00:03<00:08, 4.55MB/s]     28%|██▊       | 13.8M/49.3M [00:03<00:08, 4.45MB/s]     29%|██▉       | 14.2M/49.3M [00:03<00:08, 4.34MB/s]     30%|██▉       | 14.8M/49.3M [00:03<00:08, 4.28MB/s]     31%|███       | 15.2M/49.3M [00:03<00:08, 4.39MB/s]     32%|███▏      | 15.8M/49.3M [00:03<00:07, 4.48MB/s]     33%|███▎      | 16.2M/49.3M [00:03<00:07, 4.51MB/s]     34%|███▍      | 16.8M/49.3M [00:03<00:07, 4.54MB/s]     35%|███▍      | 17.2M/49.3M [00:04<00:07, 4.42MB/s]     36%|███▌      | 17.8M/49.3M [00:04<00:07, 4.44MB/s]     37%|███▋      | 18.2M/49.3M [00:04<00:07, 4.49MB/s]     38%|███▊      | 18.8M/49.3M [00:04<00:07, 4.50MB/s]     39%|███▉      | 19.2M/49.3M [00:04<00:06, 4.51MB/s]     40%|████      | 19.8M/49.3M [00:04<00:06, 4.56MB/s]     41%|████      | 20.2M/49.3M [00:04<00:06, 4.64MB/s]     42%|████▏     | 20.8M/49.3M [00:04<00:06, 4.61MB/s]     43%|████▎     | 21.2M/49.3M [00:04<00:06, 4.69MB/s]     44%|████▍     | 21.8M/49.3M [00:05<00:06, 4.75MB/s]     45%|████▌     | 22.2M/49.3M [00:05<00:06, 4.71MB/s]     46%|████▌     | 22.8M/49.3M [00:05<00:05, 4.67MB/s]     47%|████▋     | 23.2M/49.3M [00:05<00:05, 4.55MB/s]     48%|████▊     | 23.8M/49.3M [00:05<00:06, 4.44MB/s]     49%|████▉     | 24.2M/49.3M [00:05<00:06, 4.33MB/s]     50%|█████     | 24.8M/49.3M [00:05<00:05, 4.36MB/s]     51%|█████     | 25.2M/49.3M [00:05<00:05, 4.35MB/s]     52%|█████▏    | 25.8M/49.3M [00:05<00:05, 4.42MB/s]     53%|█████▎    | 26.2M/49.3M [00:06<00:05, 4.31MB/s]     54%|█████▍    | 26.8M/49.3M [00:06<00:05, 4.25MB/s]     55%|█████▌    | 27.2M/49.3M [00:06<00:05, 4.28MB/s]     56%|█████▋    | 27.8M/49.3M [00:06<00:05, 4.05MB/s]     57%|█████▋    | 28.2M/49.3M [00:06<00:05, 4.09MB/s]     58%|█████▊    | 28.8M/49.3M [00:06<00:05, 4.05MB/s]     59%|█████▉    | 29.2M/49.3M [00:06<00:05, 3.99MB/s]     60%|██████    | 29.8M/49.3M [00:07<00:05, 3.91MB/s]     61%|██████▏   | 30.2M/49.3M [00:07<00:04, 4.03MB/s]     62%|██████▏   | 30.8M/49.3M [00:07<00:04, 4.07MB/s]     63%|██████▎   | 31.2M/49.3M [00:07<00:04, 4.21MB/s]     64%|██████▍   | 31.8M/49.3M [00:07<00:04, 4.13MB/s]     65%|██████▌   | 32.2M/49.3M [00:07<00:04, 4.13MB/s]     66%|██████▋   | 32.8M/49.3M [00:07<00:04, 4.14MB/s]     67%|██████▋   | 33.2M/49.3M [00:07<00:04, 4.20MB/s]     68%|██████▊   | 33.8M/49.3M [00:08<00:03, 4.18MB/s]     69%|██████▉   | 34.2M/49.3M [00:08<00:03, 4.08MB/s]     71%|███████   | 34.8M/49.3M [00:08<00:03, 4.10MB/s]     72%|███████▏  | 35.2M/49.3M [00:08<00:03, 4.23MB/s]     73%|███████▎  | 35.8M/49.3M [00:08<00:03, 4.23MB/s]     74%|███████▎  | 36.2M/49.3M [00:08<00:03, 4.28MB/s]     75%|███████▍  | 36.8M/49.3M [00:08<00:03, 4.22MB/s]     76%|███████▌  | 37.2M/49.3M [00:08<00:02, 4.23MB/s]     77%|███████▋  | 37.8M/49.3M [00:09<00:02, 4.27MB/s]     78%|███████▊  | 38.2M/49.3M [00:09<00:02, 4.21MB/s]     79%|███████▊  | 38.8M/49.3M [00:09<00:02, 4.36MB/s]     80%|███████▉  | 39.2M/49.3M [00:09<00:02, 4.46MB/s]     81%|████████  | 39.8M/49.3M [00:09<00:02, 4.49MB/s]     82%|████████▏ | 40.2M/49.3M [00:09<00:02, 4.59MB/s]     83%|████████▎ | 40.8M/49.3M [00:09<00:01, 4.64MB/s]     84%|████████▎ | 41.2M/49.3M [00:09<00:01, 4.65MB/s]     85%|████████▍ | 41.8M/49.3M [00:09<00:01, 4.62MB/s]     86%|████████▌ | 42.2M/49.3M [00:10<00:01, 4.75MB/s]     87%|████████▋ | 42.8M/49.3M [00:10<00:01, 4.72MB/s]     88%|████████▊ | 43.2M/49.3M [00:10<00:01, 4.69MB/s]     89%|████████▉ | 43.8M/49.3M [00:10<00:01, 4.69MB/s]     90%|████████▉ | 44.2M/49.3M [00:10<00:01, 4.68MB/s]     91%|█████████ | 44.8M/49.3M [00:10<00:01, 4.69MB/s]     92%|█████████▏| 45.2M/49.3M [00:10<00:00, 4.72MB/s]     93%|█████████▎| 45.8M/49.3M [00:10<00:00, 4.67MB/s]     94%|█████████▍| 46.2M/49.3M [00:10<00:00, 4.70MB/s]     95%|█████████▍| 46.8M/49.3M [00:11<00:00, 4.63MB/s]     96%|█████████▌| 47.2M/49.3M [00:11<00:00, 4.56MB/s]     97%|█████████▋| 47.8M/49.3M [00:11<00:00, 4.64MB/s]     98%|█████████▊| 48.2M/49.3M [00:11<00:00, 4.64MB/s]     99%|█████████▉| 48.8M/49.3M [00:11<00:00, 4.61MB/s]    100%|█████████▉| 49.2M/49.3M [00:11<00:00, 4.62MB/s]    100%|██████████| 49.3M/49.3M [00:11<00:00, 4.44MB/s]
    The model has 3608000 trainable parameters
    Train epoch 0: TotalLoss=0.007, PSNR=9.163




.. GENERATED FROM PYTHON SOURCE LINES 365-371

Eventually, we run evaluation of the generative model by running test-time optimisation
using test measurements. Note that we do not get great results as CSGM /
AmbientGAN relies on large datasets of diverse samples, and we run the
optimisation to a relatively high tolerance for speed. Improve the results by
running the optimisation for longer.


.. GENERATED FROM PYTHON SOURCE LINES 371-373

.. code-block:: Python


    trainer.test(test_dataloader)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Eval epoch 0: PSNR=9.528, PSNR no learning=22.129
    Test results:
    PSNR no learning: 22.129 +- 2.703
    PSNR: 9.528 +- 1.301

    {'PSNR no learning': np.float64(22.128802490234374), 'PSNR no learning_std': np.float64(2.703303237720839), 'PSNR': np.float64(9.527952575683594), 'PSNR_std': np.float64(1.301279076545696)}




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 41.825 seconds)


.. _sphx_glr_download_auto_examples_adversarial-learning_demo_gan_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_gan_imaging.ipynb <demo_gan_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_gan_imaging.py <demo_gan_imaging.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: demo_gan_imaging.zip <demo_gan_imaging.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
