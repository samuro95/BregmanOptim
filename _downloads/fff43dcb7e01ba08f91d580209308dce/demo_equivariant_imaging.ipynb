{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-supervised learning with Equivariant Imaging for MRI.\n\nThis example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.\n\nThe equivariant imaging loss is presented in [\"Equivariant Imaging: Learning Beyond the Range Space\"](http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom torch.utils.data import DataLoader\nimport torch\nfrom pathlib import Path\nfrom torchvision import transforms\nfrom deepinv.optim.prior import PnP\nfrom deepinv.utils.demo import load_dataset, load_degradation, demo_mri_model\nfrom deepinv.models.utils import get_weights_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nORIGINAL_DATA_DIR = BASE_DIR / \"datasets\"\nDATA_DIR = BASE_DIR / \"measurements\"\nCKPT_DIR = BASE_DIR / \"ckpts\"\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use a subset of the single-coil [FastMRI dataset](https://fastmri.org/)\nas the base image dataset. It consists of 973 knee images of size 320x320.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We reduce to the size to 128x128 for faster training in the demo.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "operation = \"MRI\"\ntrain_dataset_name = \"fastmri_knee_singlecoil\"\nimg_size = 128\n\ntransform = transforms.Compose([transforms.Resize(img_size)])\n\ntrain_dataset = load_dataset(\n    train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True\n)\ntest_dataset = load_dataset(\n    train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of knee images and load it.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = load_degradation(\"mri_mask_128x128.npy\", ORIGINAL_DATA_DIR)\n\n# defined physics\nphysics = dinv.physics.MRI(mask=mask, device=device)\n\n# Use parallel dataloader if using a GPU to fasten training,\n# otherwise, as all computes are on CPU, use synchronous data loading.\nnum_workers = 4 if torch.cuda.is_available() else 0\nn_images_max = (\n    900 if torch.cuda.is_available() else 5\n)  # number of images used for training\n# (the dataset has up to 973 images, however here we use only 900)\n\nmy_dataset_name = \"demo_equivariant_imaging\"\nmeasurement_dir = DATA_DIR / train_dataset_name / operation\ndeepinv_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_dataset,\n    test_dataset=test_dataset,\n    physics=physics,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    num_workers=num_workers,\n    dataset_filename=str(my_dataset_name),\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)\ntest_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the reconstruction network\n\nAs a reconstruction network, we use an unrolled network (half-quadratic splitting)\nwith a trainable denoising prior based on the DnCNN architecture as an example\nof a model-based deep learning architecture from [MoDL](https://ieeexplore.ieee.org/document/8434321).\nSee :meth:`deepinv.utils.demo.demo_mri_model` for details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = demo_mri_model(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the training parameters\nWe choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)\nand the equivariant imaging loss (EI).\nThe EI loss requires a group of transformations to be defined. The forward model [should not be equivariant to\nthese transformations](https://www.jmlr.org/papers/v24/22-0315.html).\nHere we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is\nnot equivariant to rotations (while it is equivariant to translations).\n\nSee `docs <transform>` for full list of available transforms.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We use a pretrained model to reduce training time. You can get the same results by training from scratch\n      for 150 epochs.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 1  # choose training epochs\nlearning_rate = 5e-4\nbatch_size = 16 if torch.cuda.is_available() else 1\n\n# choose self-supervised training losses\n# generates 4 random rotations per image in the batch\nlosses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(n_trans=4))]\n\n# choose optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)\n\n# start with a pretrained model to reduce training time\nfile_name = \"new_demo_ei_ckp_150_v3.pth\"\nurl = get_weights_url(model_name=\"demo\", file_name=file_name)\nckpt = torch.hub.load_state_dict_from_url(\n    url,\n    map_location=lambda storage, loc: storage,\n    file_name=file_name,\n)\n# load a checkpoint to reduce training time\nmodel.load_state_dict(ckpt[\"state_dict\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verbose = True  # print training information\nwandb_vis = False  # plot curves and images in Weight&Bias\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True\n)\ntest_dataloader = DataLoader(\n    test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n)\n\n# Initialize the trainer\ntrainer = dinv.Trainer(\n    model,\n    physics=physics,\n    epochs=epochs,\n    scheduler=scheduler,\n    losses=losses,\n    optimizer=optimizer,\n    train_dataloader=train_dataloader,\n    plot_images=True,\n    device=device,\n    save_path=str(CKPT_DIR / operation),\n    verbose=verbose,\n    wandb_vis=wandb_vis,\n    show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.\n    ckp_interval=10,\n)\n\nmodel = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.test(test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}